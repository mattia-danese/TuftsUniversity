{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name and ID\n",
    "\n",
    "Mattia Danese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW05 Code\n",
    "\n",
    "\n",
    "You will complete the following notebook, as described in the PDF for Homework 05 (included in the download with the starter code).  You will submit:\n",
    "1. This notebook file, along with your COLLABORATORS.txt file and the two tree images (PDFs generated using `graphviz` within the code), to the Gradescope link for code.\n",
    "2. A PDF of this notebook and all of its output, once it is completed, to the Gradescope link for the PDF.\n",
    "\n",
    "\n",
    "Please report any questions to the [class Piazza page](https://piazza.com/tufts/spring2021/comp135).\n",
    "\n",
    "### Import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.tree\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "You should start by computing the two heuristic values for the toy data described in the assignment handout. You should then load the two versions of the abalone data, compute the two heuristic values on features (for the simplified data), and then build decision trees for each set of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Compute both heuristics for toy data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Compute the counting-based heuristic, and order the features by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counting(x_data, y_data):\n",
    "    importances = []\n",
    "    num_features = np.shape(x_data)[1]\n",
    "    i = 0\n",
    "    while i < num_features:\n",
    "        correct = 0\n",
    "        j = 0\n",
    "        while j < len(y_data):\n",
    "            if(x_data[j][i] == y_data[j]):\n",
    "                correct += 1\n",
    "            j += 1\n",
    "        i += 1\n",
    "        importances.append(1 - (correct / len(y_data)))\n",
    "    \n",
    "    return importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 0.750\n",
      "B: 0.750\n"
     ]
    }
   ],
   "source": [
    "toy_data_y = [0,0,0,0,1,1,1,1]\n",
    "toy_data_x = [[1,1], [1,1], [0,1], [0,0],\n",
    "              [0,1], [0,0], [0,0], [0,0]]\n",
    "\n",
    "importances = counting(toy_data_x, toy_data_y)\n",
    "print(\"A: %.3f\" % importances[0])\n",
    "print(\"B: %.3f\" % importances[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Compute the information-theoretic heuristic, and order the features by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy(n):\n",
    "    if n == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1 * n * np.log2(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info(x_data, y_data, n_features, n_classes):\n",
    "    gains = []\n",
    "    \n",
    "    # gets H(examples)\n",
    "    e = 0\n",
    "    for i in range(n_classes):\n",
    "        total = 0\n",
    "        \n",
    "        for y in y_data:\n",
    "            if y == i:\n",
    "                total += 1\n",
    "        total /= len(y_data)\n",
    "        \n",
    "        e += (calc_entropy(total))\n",
    "        \n",
    "    #gets remainder\n",
    "\n",
    "    for f in range(n_features):\n",
    "        \n",
    "        #gets the amount of data points at each branch (2 branches per feature)\n",
    "        branch_totals = [0,0]\n",
    "        idx_1 = []\n",
    "        idx_0 = []\n",
    "        for i in range(len(y_data)):\n",
    "            if x_data[i][f] == 1:\n",
    "                branch_totals[1] += 1\n",
    "                idx_1.append(i)\n",
    "            else:\n",
    "                branch_totals[0] += 1\n",
    "                idx_0.append(i)\n",
    "        \n",
    "        #gets the amount of data points of each class are at each branch\n",
    "        class_counts = [0 for x in range(n_classes)]\n",
    "\n",
    "        for c in range(n_classes):\n",
    "            for i in range(len(idx_0)):\n",
    "                if y_data[i] == c:\n",
    "                    class_counts[c] += 1\n",
    "\n",
    "        #calculates the entropy of branch 0\n",
    "        r0 = []\n",
    "        for i in range(n_classes):\n",
    "            r0.append(calc_entropy((class_counts[i] / branch_totals[0])))\n",
    "            \n",
    "        class_counts = [0 for x in range(n_classes)]\n",
    "        \n",
    "        for c in range(n_classes):\n",
    "            for i in range(len(idx_1)):\n",
    "                if y_data[i] == c:\n",
    "                    class_counts[c] += 1\n",
    "\n",
    "        #calculates the entropy of branch 1\n",
    "        r1 = []\n",
    "        for i in range(n_classes):\n",
    "            r1.append(calc_entropy((class_counts[i] / branch_totals[1])))\n",
    "            \n",
    "        \n",
    "        #calculates the complete remainder\n",
    "        r = ((branch_totals[0] / len(y_data)) * sum(r0)) + ((branch_totals[1] / len(y_data)) * sum(r1))\n",
    "        \n",
    "        gains.append(e-r)\n",
    "\n",
    "    return gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B: 1.000\n",
      "A: 0.311\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "toy_data_y = [0,0,0,0,1,1,1,1]\n",
    "toy_data_x = [[1,1], [1,1], [0,1], [0,0],\n",
    "              [0,1], [0,0], [0,0], [0,0]]\n",
    "\n",
    "\n",
    "importances = info(toy_data_x, toy_data_y, 2, 2)\n",
    "print(\"B: %.3f\" % importances[1])\n",
    "print(\"A: %.3f\" % importances[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Discussion of results.\n",
    "\n",
    "For the counting based heuristic, both features have equal importance as they both correctly classify 3 out of 4 data points. The information-theoretic heuristic, however, shows that feature B is more important than feature A when it comes to classifying data points. For datasets of this size, the difference, in terms of trees produced, between using both heuristics is most likely negligible, that is there probably won't be a great difference in efficiency. For much larger datasets, however, using the information-theoretic heuristic will probably save a lot of time and be more efficient as it can already identify, on a very small dataset, that feature B is more important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Compute both heuristics for simplified abalone data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Compute the counting-based heuristic, and order the features by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "height_mm: 0.413\n",
      "diam_mm:   0.298\n",
      "is_male:   0.287\n",
      "length_mm: 0.271\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "simp_train_x = np.loadtxt('./data_abalone/small_binary_x_train.csv', delimiter=',', skiprows=1)\n",
    "simp_train_y = np.loadtxt('./data_abalone/3class_y_train.csv', delimiter=',', skiprows=1)\n",
    "\n",
    "importances = counting(simp_train_x, simp_train_y)\n",
    "print(\"height_mm: %.3f\" % importances[0])\n",
    "print(\"diam_mm:   %.3f\" % importances[1])\n",
    "print(\"is_male:   %.3f\" % importances[2])\n",
    "print(\"length_mm: %.3f\" % importances[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Compute the information-theoretic heuristic, and order the features by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "height_mm:  -0.0005752180469202894\n",
      "diam_mm:    -0.002175693090761932\n",
      "is_male:    -0.0024105805899838906\n",
      "length_mm:  -0.002431707127539484\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "importances = info(simp_train_x, simp_train_y, 4, 3)\n",
    "\n",
    "print(\"height_mm: \", importances[2])\n",
    "print(\"diam_mm:   \", importances[0])\n",
    "print(\"is_male:   \", importances[1])\n",
    "print(\"length_mm: \", importances[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Generate decision trees for full- and restricted-feature data\n",
    "\n",
    "#### (a) Print accuracy values and generate tree images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy on Simplified Data: 0.7326826196473551\n",
      "Test Accuracy on Simplified Data: 0.722\n",
      " \n",
      "Train Accuracy on Full Data: 1.0\n",
      "Test Accuracy on Full Data: 0.182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'full_tree.pdf'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "simp_test_x = np.loadtxt('./data_abalone/small_binary_x_test.csv', delimiter=',', skiprows=1)\n",
    "simp_test_y = np.loadtxt('./data_abalone/3class_y_test.csv', delimiter=',', skiprows=1)\n",
    "\n",
    "model = sklearn.tree.DecisionTreeClassifier(criterion = \"entropy\")\n",
    "model.fit(simp_train_x, simp_train_y)\n",
    "\n",
    "acc_simp_train = model.score(simp_train_x, simp_train_y)\n",
    "acc_simp_test = model.score(simp_test_x, simp_test_y)\n",
    "\n",
    "print(\"Train Accuracy on Simplified Data:\", acc_simp_train)\n",
    "print(\"Test Accuracy on Simplified Data:\", acc_simp_test)\n",
    "print(\" \")\n",
    "\n",
    "dot_data = sklearn.tree.export_graphviz(model, out_file=None)\n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"simplified_tree\") \n",
    "\n",
    "#######################################################################\n",
    "\n",
    "train_x = np.loadtxt('./data_abalone/x_train.csv', delimiter=',', skiprows=1)\n",
    "train_y = np.loadtxt('./data_abalone/y_train.csv', delimiter=',', skiprows=1)\n",
    "test_x = np.loadtxt('./data_abalone/x_test.csv', delimiter=',', skiprows=1)\n",
    "test_y = np.loadtxt('./data_abalone/y_test.csv', delimiter=',', skiprows=1)\n",
    "\n",
    "model2 = sklearn.tree.DecisionTreeClassifier(criterion = \"entropy\")\n",
    "model2.fit(train_x, train_y)\n",
    "\n",
    "acc_full_train = model2.score(train_x, train_y)\n",
    "acc_full_test = model2.score(test_x, test_y)\n",
    "\n",
    "print(\"Train Accuracy on Full Data:\", acc_full_train)\n",
    "print(\"Test Accuracy on Full Data:\", acc_full_test)\n",
    "\n",
    "dot_data = sklearn.tree.export_graphviz(model2, out_file=None)\n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"full_tree\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Discuss the results seen for the two trees\n",
    "\n",
    "The training are testing accuracies of the simplified data are 0.733 and 0.722, respectively, while those of the full data are 1.0 and 0.182. The training accuracy of the full data being 1.0 shows that this tree is extremely overfit and is very precise relative to its training data. This is also supported by the testing accuracy of the full data being very low at just 0.182 as this shows that the tree is very bad a generalizing over new data (which makes sense since it is very overfit). Due to the full dataset being much bigger than the simplified dataset and that the tree of the full data is very overfit, the trees produced vary greatly in size. The tree for the simplified data has a depth of 5 and 16 leaves while the tree for the full data is large enough to the point where I cannot count the leaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
