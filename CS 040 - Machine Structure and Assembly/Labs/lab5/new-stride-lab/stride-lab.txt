                 COMP 40 Lab: Striding Through Memory
        (for groups of two -- work with your locality partner)



+--------------------------------------------------------+
|Keeper of the record: Mattia Danese                     |
|--------------------------------------------------------|
|Lab partner: Cyrus Illick                               |
+--------------------------------------------------------+


Please edit this document to include your answers to the questions
below, and use the submit40-lab-strides script to submit your
answers. 

Read these questions before you start doing the lab experiments, and
use these questions to guide your choice of test cases. Remember, the
particular tests listed in the instructions are just hints for getting
you started: you should run any experiments that you think will help
you answer these questions or understand how the cache works.

Don't worry if you aren't sure of an answer to a given quesetion.
The goal here is to start teaching you to do what cache
designers do: think step-by-step through what happens in a cache as
a program runs, use actual simulations to determine which designs
perform best on which applications, and extract general
principles of cache design from the results of these simulations.

FOR THESE FIRST FEW QUESTIONS, ASSUME A DIRECT MAPPED CACHE (the
-assoc 1 setting for testcachesim, which is the default).

Cache Size
----------

Q1a: If you know the block size in bytes for a cache and the number of
     lines in the cache, what will be the total size of the cache in
     bytes? 

    (block size) * (number of lines) = total size of cache in bytes









Q1b: For testcachesim, describe in detail how performance changes as
     the size of the cache gets larger, presuming the size of the
     test array remains the same?  

    As the size of the cache increases, the amount of cache misses 
    decreases because more elements can fit in the cache at once,
    thus the probabability of having an element already in the cache
    when the program wants to access it is higher, resulting in more
    cache hits.








Q1c. Is there a point beyond which performance stops improving as
     cache size increases? Why?

    Yes, there is a point beyond which performance stops improving as
    cache size increases. This point is when the block sizes remains constant
    but the number of lines increases such that cache size is greater than
    the amount of space needed to store all the elements because nothing 
    will be evicted and write-misses will be at their lowest point








Q1d. Sometimes performance is excellent (that is, the cache gives us a
     very good speed up) but then making the test array just a little
     bigger reduces performance dramatically. Why?


    Performance is excellent if cache size is big enough to store all the
    elements. If adding one more element causes the whole array to not 
    fit in the cache, then write-misses and evictions increase and causes
    the program to decrease in performance.








Block sizes
-----------

In this section, assume that the total size of the cache we can build
is fixed, but that we get to make choices as to whether we have
fewer lines with bigger blocks, or more lines with smaller blocks.

Q2.  Are bigger blocks always better? Worse? Assuming the total size
     of the cache remains the same, and for an application like
     testcachesim, which block size is best?


    Small blocks, with more lines, in the cache results in a lower 
    eviction amount because when a line is evicted, only a 
    small amount of elements would no longer be in the cache, rather 
    than one line with a large block, which would result in all of the 
    elements being removed from the cache.

    Large blocks, with less lines, decreases the amount of write-misses 
    because more elements are stored in the cache once one element is 
    accessed.

    For the 'testcachesim' application, bigger block sizes would increase
    performance if the elements, presumably stored in an array, are accessed
    sequentially. Otherwise, smaller blocks with more lines would be best.






Writing data
------------

Q3.  Reread the part of the instructions that explains the
     "Reads_for_writes" count in your cache statistics. Is there a
     value of the block size that will make "Reads_for_writes" zero?
     If you understand this, then you understand a lot about how
     "write-back" caches work.


    No, because the 'testcachesim' program writes to an element before 
    it reads it. Thus, when the program writes to the first element, 
    which is not in the cache yet, the program will have to read it in
    first.






Q4.  Explain why, for applications that update memory repeatedly, a cache that
     performs better may finish with more dirty blocks than a cache
     that does not perform well on the same application.

    A dirty block is data that has been updated in the cache, but not in 
    main memory. If there are more dirty blocks, then less blocks had to 
    be evicted because they then would have updated main memory upon eviction.
    Thus, less evictons means a better performance. 




=================================================================
                     Associative caches
=================================================================

Q5.  Can you describe a situation in which a fully associative cache
     will outperform a direct-mapped cache?


    If you are accessing many elements in the same 'zone', then a fully
    associative cache will perform better because it will presumably
    have a lot less evictions than a direct-mapped cache.








Submit this file using script

       submit40-lab-strides
